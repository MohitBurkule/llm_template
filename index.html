<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Riddle AI with WebLLM</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    /* Global Styles */
    body {
      font-family: 'Arial', sans-serif;
      background: linear-gradient(135deg, #2b2b2b, #1a1a1a);
      color: #f4f4f4;
      margin: 0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    h1, p {
      text-align: center;
    }
    /* Containers */
    .download-container, .riddle-container {
      width: 90%;
      max-width: 800px;
      margin: 20px 0;
    }
    .hidden {
      display: none;
    }
    /* Chat Box */
    .chat-box {
      background: linear-gradient(135deg, #3e3e3e, #333);
      width: 100%;
      height: 600px; /* Increased height for a bigger chat window */
      border-radius: 10px;
      overflow-y: auto;
      padding: 15px;
      margin: 10px 0;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.6);
    }
    .chat-stats {
      background-color: #4c4c4c;
      padding: 10px;
      margin-top: 10px;
      border-radius: 5px;
      font-size: 0.85em;
    }
    .chat-input-container {
      display: flex;
      align-items: center;
      gap: 10px;
      margin-top: 15px;
    }
    .config-options {
      margin-top: 10px;
      font-size: 0.9em;
    }
    .button-bar {
      margin: 10px 0;
      display: flex;
      justify-content: center;
      gap: 10px;
    }
    /* Message Styles */
    .message-container {
      margin: 8px 0;
      padding: 10px;
      border-radius: 8px;
    }
    .assistant {
      background-color: #555;
      text-align: left;
    }
    .user {
      background-color: #2f2f2f;
      text-align: right;
    }
    .message {
      white-space: pre-wrap;
      word-wrap: break-word;
    }
    /* Input & Buttons */
    input[type="text"] {
      flex: 1;
      padding: 10px;
      border-radius: 5px;
      border: none;
      font-size: 1em;
    }
    button {
      padding: 10px 15px;
      border: none;
      border-radius: 5px;
      background-color: #0066cc;
      color: #fff;
      cursor: pointer;
      font-size: 1em;
      transition: background-color 0.2s;
    }
    button:hover:not(:disabled) {
      background-color: #005bb5;
    }
    button:disabled {
      background-color: #555;
      cursor: not-allowed;
    }
    label {
      font-size: 1em;
    }
  </style>
</head>
<body>
<h1>Riddle AI with WebLLM</h1>

<!-- Model Control -->
<div class="download-container">
  <div class="button-bar">
    <select id="model-selection"></select>
    <button id="download">Download Model</button>
    <button id="reload">Reload Model</button>
    <button id="clear-chat">Clear Chat</button>
  </div>
  <p id="download-status" class="hidden"></p>
</div>

<!-- Riddle & Chat -->
<div class="riddle-container">
  <p><strong>Riddle:</strong> I speak without a mouth and hear without ears. I have no body, but I come alive with wind. What am I?</p>
  <div class="chat-container">
    <div id="chat-box" class="chat-box"></div>
    <div id="chat-stats" class="chat-stats hidden"></div>
    <div class="chat-input-container">
      <label for="user-input">My guess is:</label>
      <input type="text" id="user-input" placeholder="Enter your guess..." />
      <button id="send" disabled>Submit</button>
    </div>
    <div class="config-options">
      <label>
        <input type="checkbox" id="history-checkbox" checked>
        Include conversation history in AI prompt
      </label>
    </div>
  </div>
</div>

<script type="module">
  import * as webllm from "https://esm.run/@mlc-ai/web-llm";

  // Preserve your working system prompt exactly.
  const initialSystemMessage =     {
    role: "system",
    content: "You are a riddle answer checking AI. The riddle is: 'I speak without a mouth and hear without ears. I have no body, but I come alive with wind. What am I?' The correct answer is 'echo'. When a player submits a guess compare the guess case-insensitively to 'echo'. If the guess is exactly 'echo', reply with 'Correct!' . Otherwise, reply only with a brief hint related to the riddle without revealing the answer or adding extra commentary.Never reveal the answer 'echo', only explain why the players answer is incorrect and how it does not fulfill one or more of the riddle's requirements."
  };

  // Conversation messages. Start with the system prompt.
  let messages = [initialSystemMessage];

  // Filter models: only include those with "q4f32" (adjust as needed).
  const availableModels = webllm.prebuiltAppConfig.model_list
          .filter(m => m.model_id.includes("q4f32"))
          .map(m => m.model_id);
  let selectedModel = availableModels[0] || "SmolLM2-360M-Instruct-q4f32_1-MLC";

  const engine = new webllm.MLCEngine();

  function updateEngineInitProgressCallback(report) {
    console.log("initialize", report.progress);
    document.getElementById("download-status").textContent = report.text;
  }
  engine.setInitProgressCallback(updateEngineInitProgressCallback);

  async function initializeWebLLMEngine() {
    document.getElementById("download-status").classList.remove("hidden");
    selectedModel = document.getElementById("model-selection").value;
    const config = {
      temperature: 0.2, // Lower temperature for deterministic output.
      top_p: 1,
    };
    await engine.reload(selectedModel, config);
  }

  // Auto-download routine: wait until the model list is available then populate and load.
  window.addEventListener("load", async () => {
    // Wait until the model list is loaded.
    while (!webllm.prebuiltAppConfig ||
    !webllm.prebuiltAppConfig.model_list ||
    webllm.prebuiltAppConfig.model_list.length === 0) {
      await new Promise(resolve => setTimeout(resolve, 100));
    }
    populateModelSelection();
    await initializeWebLLMEngine();
    document.getElementById("send").disabled = false;
  });

  async function streamingGenerating(streamMessages, onUpdate, onFinish, onError) {
    console.log("Streaming with messages:", streamMessages);
    try {
      let curMessage = "";
      const completion = await engine.chat.completions.create({
        stream: true,
        messages: streamMessages,
      });
      for await (const chunk of completion) {
        const curDelta = chunk.choices[0]?.delta?.content;
        if (curDelta) {
          curMessage += curDelta;
        }
        onUpdate(curMessage);
      }
      const finalMessage = await engine.getMessage();
      onFinish(finalMessage);
    } catch (err) {
      onError(err);
    }
  }

  function onMessageSend() {
    const input = document.getElementById("user-input").value.trim();
    if (input.length === 0) return;
    document.getElementById("send").disabled = true;

    // Prepend "my guess is " to user's submission.
    const userContent = "my guess is " + input;
    const userMessage = { role: "user", content: userContent };
    messages.push(userMessage);
    appendMessage(userMessage);

    document.getElementById("user-input").value = "";
    document.getElementById("user-input").setAttribute("placeholder", "Checking your guess...");

    // Display a temporary "thinking..." message in the UI.
    const tempElement = { role: "assistant", content: "thinking..." };
    appendMessage(tempElement);

    // Determine if history should be sent.
    const useHistory = document.getElementById("history-checkbox").checked;
    let streamMessages = useHistory ? messages : [messages[0], userMessage];

    streamingGenerating(
            streamMessages,
            (partialResponse) => { updateLastMessage(partialResponse); },
            (finalMessage) => {
              updateLastMessage(finalMessage);
              messages.push({ role: "assistant", content: finalMessage });
              document.getElementById("send").disabled = false;
              document.getElementById("user-input").setAttribute("placeholder", "Enter your guess...");
              engine.runtimeStatsText().then((statsText) => {
                document.getElementById("chat-stats").classList.remove("hidden");
                document.getElementById("chat-stats").textContent = statsText;
              });
            },
            console.error
    );
  }

  function appendMessage(message) {
    const chatBox = document.getElementById("chat-box");
    const container = document.createElement("div");
    container.classList.add("message-container");
    const newMessage = document.createElement("div");
    newMessage.classList.add("message");
    newMessage.textContent = message.content;
    container.classList.add(message.role === "user" ? "user" : "assistant");
    container.appendChild(newMessage);
    chatBox.appendChild(container);
    chatBox.scrollTop = chatBox.scrollHeight;
  }

  function updateLastMessage(content) {
    const messagesDOM = document.getElementById("chat-box").querySelectorAll(".message");
    const lastMessageDOM = messagesDOM[messagesDOM.length - 1];
    lastMessageDOM.textContent = content;
  }

  function populateModelSelection() {
    const modelSelect = document.getElementById("model-selection");
    availableModels.forEach((modelId) => {
      const option = document.createElement("option");
      option.value = modelId;
      option.textContent = modelId;
      modelSelect.appendChild(option);
    });
    modelSelect.value = selectedModel;
  }

  // Clear chat function resets the chat box and messages (preserving the system prompt).
  function clearChat() {
    document.getElementById("chat-box").innerHTML = "";
    messages = [initialSystemMessage];
  }

  // Reload model button: reinitializes the engine.
  async function reloadModel() {
    document.getElementById("download-status").classList.remove("hidden");
    await initializeWebLLMEngine();
  }

  // Button event listeners.
  document.getElementById("send").addEventListener("click", onMessageSend);
  document.getElementById("clear-chat").addEventListener("click", clearChat);
  document.getElementById("reload").addEventListener("click", reloadModel);
</script>
</body>
</html>
